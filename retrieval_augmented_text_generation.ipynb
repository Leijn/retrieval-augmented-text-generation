{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwTBxQNy2_hH",
    "outputId": "7239f940-43d3-4b65-abd4-74acfac2c2c9"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/pubmedqa/pubmedqa/refs/heads/master/data/ori_pqal.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7cJ6GW5Yjzp",
    "outputId": "7ee5a317-093b-4464-b226-8f2a401061c9"
   },
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface\n",
    "!pip install -qU langchain-text-splitters\n",
    "!pip install -U langchain-community\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8kWDBg93hCd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tmp_data = pd.read_json(\"/content/ori_pqal.json\").T\n",
    "# some labels have been defined as \"maybe\", only keep the yes/no answers\n",
    "tmp_data = tmp_data[tmp_data.final_decision.isin([\"yes\", \"no\"])]\n",
    "\n",
    "documents = pd.DataFrame({\"abstract\": tmp_data.apply(lambda row: (\" \").join(row.CONTEXTS+[row.LONG_ANSWER]), axis=1),\n",
    "             \"year\": tmp_data.YEAR})\n",
    "questions = pd.DataFrame({\"question\": tmp_data.QUESTION,\n",
    "             \"year\": tmp_data.YEAR,\n",
    "             \"gold_label\": tmp_data.final_decision,\n",
    "             \"gold_context\": tmp_data.LONG_ANSWER,\n",
    "             \"gold_document_id\": documents.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NyNYCSxa3mtj",
    "outputId": "48760280-1b4a-494f-82fb-9d64bed18821"
   },
   "outputs": [],
   "source": [
    "questions.iloc[0].question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "WzBKcjYc3qBE",
    "outputId": "c5b41988-4252-48c3-868a-51c3f74ef3d4"
   },
   "outputs": [],
   "source": [
    "documents.iloc[0].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZEwwQzR-Cai"
   },
   "outputs": [],
   "source": [
    "# Step 1: Configure your LangChain LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oWBkD4W3Svg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "0568479cdd7144efb8c11c6d33ae31e1",
      "e22c966ebd704ad1b202150832b3a333",
      "0c60431454df404b95479de9df621cbd",
      "3b1b26714aac45259ffb70fda4af4cdc",
      "e85342aa43764dbe85bde9ea98b90315",
      "8e2a856e3c1b4de7a8ae243e94246692",
      "0ba7dacf4a8f4ef28e0d01dae571e51a",
      "f1326e7a2b5640608b483edc4a720b96",
      "47d1d75bfdda4060ad652ea80c042fa5",
      "9e6e189a50024170a39798d70ff86ee7",
      "319ca7d935a64dc09c51bca5243e41b6",
      "8fa5d4c361a44d6f83cf2a69e55f405d",
      "ec611564730646169e784868f8a8beb7",
      "d77089f946b341bd9456c0f167b9fe0b",
      "d623674705184f7abd5fdb8c58fec432",
      "c70497d3084d4c6488e85b422ca2cef8",
      "3ce8eb35346b4318b66fd9e65d1591d7",
      "eb63738505cf4014a4ce2c87a65da3fc",
      "d9048a5a5cea49a5a64cb2b3b4679895",
      "f6854229f3984318af7b9403c8e4c53e",
      "f62871fc0f2c4c0f97a601fa833e75a0",
      "d0916f05f54142dc8976e4b2999346c4",
      "b2ddfd4d82ec416e821b248ead87f41c",
      "2ee776a31a684c2997330f502a3397b8",
      "295b8968b54846ba9729a37b52ccb10a",
      "1b38f310471f4a929afb6180eb7cf0ee",
      "74104867f39d4a86ae60b26d3d785897",
      "e545fb3260d5463394a86e75c4f8c525",
      "507fd5961fc64e4487d47f49a4d0caa7",
      "ec9117d23cab4c4fb75980d539b55dfa",
      "d73006c168b440d49b82eb564de3de29",
      "cdee6161db034f74b89df0f51a5bcab5",
      "5ce31903e56a4196bf412d50e9d0240a",
      "9012e7c2d2574bb082b667c88740d56d",
      "49fd35fcc3434d68adfa9f201a1abc04",
      "cd770754c6f9485190c8f30bf9a7a463",
      "821eff6b4f9c4427ba9b0a7bc5511fb4",
      "96279afb6be347148dd5e36f0cbeff1f",
      "ba9c524816c84c7b9d939427bdd2fd4a",
      "71ad5302af7b41b58b97656e8a820286",
      "fea969f94d8747c1b45062ccf2934f28",
      "8f4850a571ec424e804df70c851422f0",
      "21c3501c5c25487ca99ca32c94cfa17a",
      "78a75f0cb1e74bdb9869ac43a9cb39cc",
      "6023273c321144a3b8292931c8cce27d",
      "8efe6bcf0e7b4242994accc549217c9b",
      "a93c51996b2c47e5a6a57033fff9431b",
      "0b3c3cb3fc54430ab8d9a5716f86856d",
      "ba59b4137df742a3820b4c58575de210",
      "68da550b964049edbfe1780330f36de0",
      "6cf40b30455740528d79b6ee033d41dc",
      "4cfc1f070a6c441384d98e0e3eaaf38f",
      "395af799a7f8497b97b5834ff8f5ed66",
      "55cbeaa5e4d4427aa0efec6bf16070c2",
      "ddff9090cf8e4a989f3544564e9c4d2e",
      "122d9c98357f4896a8f782659a2895a5",
      "137d7ae672df44a98ea189b2a44658eb",
      "bada3b6d967a40ac89031a600cd50ca1",
      "8620630ebfa8499d841f16b4c05ecde9",
      "51ddb4f3b65643cea2b29fbc59d93357",
      "144f1838d2704ec481375b43688a9bd8",
      "bf0eaedef1be42edb7726d25239f63aa",
      "6ead7b57d73b46f4868b6ffeb454fc6f",
      "4c56acc056364bf89454f20c5595b071",
      "9d8ffd3cf28e4831b841d11762ef8738",
      "3b56df381ea44d22871720f736796589"
     ]
    },
    "id": "zGYN7oWFyYHe",
    "outputId": "ccf52ea9-0f39-4158-aed8-bc09c716dc6b"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "# Load the pipeline using LangChain's HuggingFacePipeline\n",
    "lm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",     # Define task\n",
    "    device= 0,           # Set to 0 for GPU support\n",
    "    model_kwargs={\"torch_dtype\": \"auto\", \"pad_token_id\": tokenizer.eos_token_id,},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 40},    # Set generation parameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWSK3PFBxHI2",
    "outputId": "2049a446-f431-4da8-b35f-51aa9efb112a"
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "prompt = \"What is programmed cell death?\"\n",
    "output = lm(prompt)\n",
    "print(\"Generated Output:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekyYbGvXG24A"
   },
   "outputs": [],
   "source": [
    "# Step 2: Set up the document database and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWu4P_3JwpRH"
   },
   "outputs": [],
   "source": [
    "# Step 2.1: Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425,
     "referenced_widgets": [
      "535a89bff46a44a9b7a59cfd2d70711c",
      "b3a85586b2754932aa39d5a91b9c3e40",
      "a14f1c7e673744fca332641bf2f7c22e",
      "a43e4b8571944a1fa380be139a9f07eb",
      "07f2fe71c7e14e49b4375e338b46aec0",
      "197c8f7402e74ec38b5174b1611bbe1d",
      "0d88ab453fb14a10a944f42698bdc63d",
      "462940b1745349e2bb9109990d7471d2",
      "dbb99427bec34aeca4fa1f88936a4770",
      "6fd16b48a9804322bc50c47e201cf0c9",
      "91475ff017df49059ed5415fa8100abb",
      "7530c7df0f14471ea20e84b1412bcec2",
      "272289701dd14911bf4d95bf512940f8",
      "a2294aa0485043edbda69d60af57b89b",
      "43df6c86a336449e80c3b751eeb06859",
      "2a583ad3279c4a39a29ac7f3a0bb5000",
      "5bbac496b2d14749877138c8134cd52d",
      "a85c1ad54f8b469990589afd21394a02",
      "dec5b09ab9134bada7ef17a5380f3daa",
      "5c1a143dbfea4fb195ddaea6b62ab749",
      "196ba10489844d308f46d6534f8db515",
      "717ef08572994adb8cd6bcbf4160b23b",
      "1284b17819e04a759e3c88dcd845072a",
      "c14ef0f710674f00a3b3d37ad46f9aaa",
      "e6bd4851fa824da283411c873855acc4",
      "c8483761f3b74034a2cb3acde183fa00",
      "7c87e0e360254cb69fc5b821c947b660",
      "8d0e0b9625b049eab7fdf511e2708405",
      "549ec793acb84355b15d40378654c1dc",
      "3ad0684995e44ec6aed38566227010aa",
      "297ea5ac4fe945d790b7d98106b7da93",
      "675cf1fc02fe458b93743212e59ac347",
      "123c7c80b8ff46b3a97ac7fb7ddfc29e",
      "453036229f87479483e7b1c46c561649",
      "242ce3b5f9e74f038cc1d6891e63845c",
      "c7615f354dab40628439541e7ed121ed",
      "5ceff27c1be541f28ccb60d369e039ca",
      "cd09d246099c43d4958c47624d701543",
      "878491a572a2436fb02d8f93d1f65610",
      "25f45d53f59c427c930dd3e7e7e64a66",
      "309c1aacf9234e01877289541af939bf",
      "2ae439d53f814caabe9b23e9dc0c7158",
      "89f544e43a9840ca9c8ec7533faa62e5",
      "879d1881e8984a329e46a1d1a1890506",
      "a9235e8d72de45819404f1688c712d4b",
      "ac74cb7bd0b24deaac6eccb10343452d",
      "101461bceb744cf3b248c6744a97c12e",
      "9ae2f8aa33364ba58ffbe484817c6821",
      "f85213fde27a47baad1d0238f19ccbed",
      "dd45b725127247739f890edee9f26a85",
      "588daa96e1b84cf5a4c1ba5b54ec1361",
      "67bef96040224ec8933cd3deb8709e31",
      "2425a684c4ff490dae5401cb432f0f45",
      "08760e18e3c24c4bb68bf4dae55974aa",
      "ca9e663326f448289ac7f73baebcecc8",
      "3058c1d41ae34889b7edcd1e8c4117a3",
      "f597c03d9bfe4d6a830192e3a5488121",
      "93b6c1eadb4649bf819b9b8b0efa28e3",
      "46d54d51872d45b3a5e113b439a6c746",
      "80d561fca69a4125a1e282f89220f3bf",
      "738a8e7774d94e89a331f14bb2a09af3",
      "5209ec20a7904f8ebcc1dbc81c90fd32",
      "23215811da3142918d91e1324a817e08",
      "607710ce250746659cf9ff0d76f95e5c",
      "2a1f2772bda24c4eb05467f291af8330",
      "c2cfee27ca604362922d69f2c611ecde",
      "ca4bb3c7cf8447b9bde67cce67548cba",
      "e58654f766dc492dbfe2eab7ec238267",
      "b0a68338c80f40e985c23970292a33af",
      "46c2f9c005394ef4bbaef01f510be187",
      "2b547d704e4c42cbaa914682aa38d953",
      "5a159d21a34d48d5b3c98e267c2fd536",
      "16647511de8f43799b2346a75851dcd0",
      "aacb1ae37bb745c18b0444042e9ef526",
      "687fad737eaf4990800bb9e980d497fa",
      "4c491280daa643cdb9f14bf9a58b415b",
      "0f96247c5ad34500951f34762190f1cd",
      "3068e9eb4ea243f289722b299511c709",
      "3cdc41d0791a43a288b433da4d3c910b",
      "f6ffadce53c34012bb30c76d2abd5904",
      "8090d67fa60b43078dd1f2befb620f66",
      "4522ac42e7db45f8afadf86a63baf64b",
      "0fa542c4ec73484c9fc1b433c876ea5f",
      "66d24b66688c4a8281a598489ec1c436",
      "91ec3a99c4d74730a08381c2d36f0f24",
      "247e283319ea4d90b6754ab53880c57b",
      "5466e50f7e27487e953b20159506d077",
      "d0952f3a819f428380eac71bbdc49c13",
      "c3746e75fb2d4508b66e6b877978b8fc",
      "d8c1361447864cf982bcacf8d21b20a6",
      "105bc5080f634cd6ab29551442affc57",
      "b69a8cfce053492285ecd852c2624bd4",
      "1019d2b6c7054942903280285f9f993a",
      "53db078c895d49e38dc52335c59153ef",
      "ce3e4f5075304491afc9f2fc2ef5417d",
      "b4074b7391a34390835eac8dd7c11826",
      "1f8b15adf1c64459b039018ed0bbf095",
      "df911c3b720d44eca01c789ea22951ca",
      "05daafc58f594cc1826ea9dad8cf0dad",
      "79cd01e847b9465991be87400cf7ac93",
      "59b47007669f4cf9879477b86f5b0675",
      "22904563d53740b88c51792f91f984ac",
      "27c23ebeb34840658b163cbbfdb69260",
      "c86c8a5835a2455289b6fd72275ce17e",
      "5b54c2560ac249388feadc8a4671035e",
      "8b9cb8fab52e4cf7bf6f4258fe2d8899",
      "7c988b353d9448a187aa520b8d560689",
      "f586c6adb5294cec8b180231dfed9548",
      "0664601a128f4800a665e674b0799439",
      "f3b6c9dd2b0e41a4b1486b16dafaf3e8",
      "dcdff53739bb41038fa0e9f13e1bcdd7",
      "6126d45414fd425cabff698fffed904d",
      "4ffec27222cc4a61a84eccbd53e44283",
      "d5beb5c9e3c54d0bb2454fc39a12d3b0",
      "d12fb318911948f7844000fed188389e",
      "a4e12ecec46d4182b2d3a08a1d32c262",
      "1052f8391d2243d882d0741e42dd9b35",
      "10fb7fd263c148759908ef470ba787aa",
      "1990bd5279aa46709df80317af39ec73",
      "7a7f8078c1fe489e90cb174a57a393f0",
      "8869bbd0bdbd4226bbb700290070a530"
     ]
    },
    "id": "uGLYVdnf2Tg_",
    "outputId": "6ce3c892-6b50-4006-966d-2b8de9625fc4"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8DFDI8v2uGv",
    "outputId": "99e0aa8a-5cb4-48e9-a52c-d942aa1b70cc"
   },
   "outputs": [],
   "source": [
    "# Sanity check: check the embedding shape\n",
    "sample_text = \"Programmed cell death is a vital process in biological organisms.\"\n",
    "embedding = embedding_model.embed_query(sample_text)\n",
    "\n",
    "print(f\"Embedding shape: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV6GTzHUMxrW"
   },
   "outputs": [],
   "source": [
    "# Step 2.2: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Yk2uHUFV6M0",
    "outputId": "b6fbfc57-c1c1-40c1-bd1c-40e0a55ee64c"
   },
   "outputs": [],
   "source": [
    "# compute the avergae length of the gold context to chose suitable chunk size\n",
    "gold_context_lengths = [len(context) for context in questions['gold_context'].tolist()]\n",
    "\n",
    "average_length = sum(gold_context_lengths) / len(gold_context_lengths)\n",
    "\n",
    "print(f\"Average Length of Gold Context: {average_length:.2f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaxHGeoxNNeR",
    "outputId": "3f9dc125-d3aa-403c-dcec-48fb370f2cc4"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,       # Maximum characters in each chunk\n",
    "    chunk_overlap=30      # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Chunk the documents\n",
    "chunked_documents = []    # Store chunks as a list of dictionaries\n",
    "for idx, row in documents.iterrows():\n",
    "    chunks = text_splitter.split_text(row['abstract'])  # Split the abstract\n",
    "    for chunk in chunks:\n",
    "        chunked_documents.append({\"doc_id\": idx, \"chunk\": chunk})\n",
    "\n",
    "# Print a few samples for sanity check\n",
    "print(\"Sample chunks:\")\n",
    "for i in range(10):\n",
    "    print(f\"Doc ID: {chunked_documents[i]['doc_id']}\")\n",
    "    print(f\"Chunk: {chunked_documents[i]['chunk'][:]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V38TrlvOwPb"
   },
   "outputs": [],
   "source": [
    "# Step 2.3: Define a vector store and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2rrVCXAXSPH"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "texts = [chunk[\"chunk\"] for chunk in chunked_documents]\n",
    "metadatas = [{\"doc_id\": chunk[\"doc_id\"]} for chunk in chunked_documents]\n",
    "\n",
    "# Initialize Chroma vector store\n",
    "vector_store = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUyiZI2o3wUQ",
    "outputId": "8e96b528-74d1-49eb-e872-cf5b57481560"
   },
   "outputs": [],
   "source": [
    "# Perform similarity search with scores\n",
    "query = \"What is programmed cell death?\"\n",
    "# query = \"Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\"\n",
    "results = vector_store.similarity_search_with_score(query, k=3)\n",
    "\n",
    "# Display results\n",
    "print(\"Query Results:\")\n",
    "for res, score in results:\n",
    "    content_excerpt = res.page_content[:600]  # Limit output the characters for readability\n",
    "    print(f\"* [SIM={score:.3f}] {content_excerpt} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcukjdxiPYdm"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define the full RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7TEviwY43oj"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "rag_prompt = ChatPromptTemplate.from_template(\n",
    "    template=\"Based on the following retrieved context, answer the question with 'Yes' or 'No' only. \"\n",
    "             \"Do not provide any explanation or additional information. If the context does not contain enough evidence, answer 'No'. \"\n",
    "             \"Your answer must be a single word: 'Yes' or 'No'.\\n\\n\"\n",
    "             \"Context: {context}\\n\"\n",
    "             \"Question: {question}\\n\\n\"\n",
    "             \"Answer (Yes or No):\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqWAjL5F5oHh"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "lm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    device= 0,\n",
    "    model_kwargs={\"torch_dtype\": \"auto\", \"pad_token_id\": tokenizer.eos_token_id,},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 1},    # Set generation parameters\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever,\n",
    "    llm=lm,\n",
    "    chain_type=\"stuff\",          # Combine context and LM generation\n",
    "    return_source_documents=True,     # Return retrieved documents\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijAVdb95E-Ek",
    "outputId": "571d166d-0a64-4c49-95a0-c62a58703a00"
   },
   "outputs": [],
   "source": [
    "# Test a single question\n",
    "test_input = {\"query\": \"What is programmed cell death?\"}\n",
    "\n",
    "# Run the RAG pipeline for the test input\n",
    "response = rag_chain.invoke(test_input)\n",
    "\n",
    "\n",
    "print(f\"--- Test Question ---\")\n",
    "print(f\"Answer: {response['result']}\")\n",
    "print(f\"Retrieved Document: {response['source_documents']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08eKhSrRbEXN"
   },
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the RAG pipeline on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YqoT5QGbJ1T"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Function to evaluate retriever accuracy\n",
    "def evaluate_retriever(all_doc_ids, gold_document_ids):\n",
    "    # Count matches between retrieved and gold document IDs\n",
    "    correct_matches = 0\n",
    "    total_queries = len(gold_document_ids)\n",
    "\n",
    "    for i in range(total_queries):\n",
    "        if gold_document_ids[i] == all_doc_ids[i]:\n",
    "            correct_matches += 1\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_matches / total_queries\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Function to evaluate generator using F1 score\n",
    "def evaluate_generator(predictions, gold_labels):\n",
    "    predictions_binary = [1 if pred.lower() == \"yes\" else 0 for pred in predictions]\n",
    "    gold_binary = [1 if label.lower() == \"yes\" else 0 for label in gold_labels]\n",
    "    return f1_score(gold_binary, predictions_binary)\n",
    "\n",
    "# Function to extract answer\n",
    "def extract_answer_from_result(result, marker):\n",
    "    marker_index = result.lower().find(marker)\n",
    "\n",
    "    if marker_index != -1:\n",
    "        # Extract the answer portion\n",
    "        answer_start = marker_index + len(marker)\n",
    "        answer = result[answer_start:].strip().split(\"\\n\")[0].strip()  # Extract and clean the first line\n",
    "\n",
    "        # Validate if the answer is 'yes' or 'no'\n",
    "        if answer.lower() in [\"yes\", \"no\"]:\n",
    "            return answer.lower()   # Return 'yes' or 'no' in lowercase\n",
    "        else:\n",
    "            return \"invalid\"     # Return 'invalid' if the answe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X736RHLhVcO_"
   },
   "outputs": [],
   "source": [
    "def compute_performance(rag_chain):\n",
    "  responses = []\n",
    "  predictions = []\n",
    "  all_doc_ids = []\n",
    "\n",
    "  for idx, row in questions.iterrows():\n",
    "\n",
    "      # Input question to RAG pipeline\n",
    "      test_input = {\"query\": row['question']}\n",
    "      response = rag_chain.invoke(test_input)\n",
    "\n",
    "      doc_id = response['source_documents'][0].metadata['doc_id']\n",
    "      cleaned_result = extract_answer_from_result(response['result'], marker = \"answer (yes or no):\")\n",
    "\n",
    "      responses.append(response)\n",
    "      all_doc_ids.append(doc_id)\n",
    "      predictions.append(cleaned_result)\n",
    "\n",
    "  return responses, predictions, all_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unDBCCVI8cF-"
   },
   "outputs": [],
   "source": [
    "# Test a single question\n",
    "# test_input = {\"query\": questions.iloc[5]['question']}\n",
    "# response = rag_chain.invoke(test_input)\n",
    "# print(f\"--- Test Question ---\")\n",
    "# print(f\"Answer: {response['result']}\")\n",
    "# print(f\"Retrieved Document: {response['source_documents']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQeKwgcfVN8b",
    "outputId": "1971051a-dd53-4dab-8377-a081ec45992a"
   },
   "outputs": [],
   "source": [
    "responses, predictions, all_doc_ids = compute_performance(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuTet_ho9mLQ",
    "outputId": "e8521775-0e91-4591-f3bf-45b93c4aab57"
   },
   "outputs": [],
   "source": [
    "gold_context = questions['gold_context'].tolist()\n",
    "print(gold_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUveARGeWwDM",
    "outputId": "c17dbfd1-3933-4ba0-dc8f-671d2e451586"
   },
   "outputs": [],
   "source": [
    "# Evaluate F1 score\n",
    "gold_labels = questions['gold_label'].tolist()\n",
    "f1 = evaluate_generator(predictions, gold_labels)\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nDoqw3lWhIZ",
    "outputId": "7a2f8102-5649-4fb0-f22e-de403e4adb2e"
   },
   "outputs": [],
   "source": [
    "# Evaluate retriever accuracy\n",
    "gold_document_ids = questions['gold_document_id'].tolist()\n",
    "accuracy = evaluate_retriever(all_doc_ids, gold_document_ids)\n",
    "print(f\"Retriever Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7EbAOTQX6TV"
   },
   "outputs": [],
   "source": [
    "# Baseline - without context\n",
    "\n",
    "responses_baseline = []\n",
    "predictions_baseline = []\n",
    "\n",
    "for idx, row in questions.iterrows():\n",
    "    # Input question to RAG pipeline\n",
    "    prompt = f\"Answer the question with 'Yes' or 'No' only: {row['question']}\\nAnswer:\"\n",
    "    response = lm.invoke(prompt)       # Directly use the LM without context\n",
    "\n",
    "    cleaned_result = extract_answer_from_result(response, marker = \"answer:\")\n",
    "\n",
    "    responses_baseline.append(response)\n",
    "    predictions_baseline.append(cleaned_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8puKabaycemH",
    "outputId": "b9684589-68d3-4b29-b1cb-f295f7290bff"
   },
   "outputs": [],
   "source": [
    "# Evaluate F1 score for Non-context\n",
    "f1_baseline = evaluate_generator(predictions_baseline, gold_labels)\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fA4cQxAMky93"
   },
   "outputs": [],
   "source": [
    "# Step 5: Make improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVv_R6D8-NKt"
   },
   "outputs": [],
   "source": [
    "#====================================================\n",
    "# Impromentt 1 - Change Embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "c85ba570848e4d8fb17542a1f533c899",
      "615e9335ac934d70bd2e4d1d209f54cb",
      "e67f0044e741456fbea5534ff3cdc1d0",
      "322b39b0dcdd4be2aa04e6c246106587",
      "cf684c2fd59c49c4b08e113e7d414cfd",
      "5b343bf40f6c475388a99b39d95a8274",
      "709d89d4909d48998e1ac74a542a0b4f",
      "3a57ed345e1d48ef88785b7449192052",
      "ce97dc8aa61243cfbb0abecfb2c884d9",
      "593a793f6e5643eebc88a3c4d1283b24",
      "05368072047f42428b618ff5e20373f8",
      "aaea18a75f1b49638702261089eed8e7",
      "52f9a589bc1548329860cf6475ab52d5",
      "f2f3f664bfd64856a0b5923b5f4dfd0c",
      "244891ccf2f74139be05d54116b707aa",
      "03568cc6b09d41e8bbba381d187fbdb2",
      "f1ae7d62127f48e392ded75b64f8ffcf",
      "15b572dfdb2743d683bf5bd9ad3c3659",
      "41cf4160ff3c486791e658588613355c",
      "d13b3c4719ba4648bd826c5a8278f12b",
      "82afbecb0441406fb01cce66e6b4846e",
      "11dba03490c042bb952d08f190ae205f",
      "7717d98f877746d4bd589dc21ac16595",
      "3e6e451f368441719bff420b3a77104a",
      "a034045d860e42ea8629a36e3a313a2c",
      "ace5fd1dc3b34ef68cedc8785f9d9667",
      "a6cb2aa568e94f928331a5b7703fa4af",
      "ea6897cf3894423b8cc594efeaeeadb5",
      "8a47cbf6594b418b8a30764389da568d",
      "cddf2462ac534bd597eec2aa81aad3b8",
      "dd5f5cf5342f420d8cc7e98acc4c5d7f",
      "f8d7cdba17dc46da97f2e1f38555a7cf",
      "05f5be0404534370bf00ab7ee00d12b1",
      "a388f871281944acadb6260082d91054",
      "fc897e9c82f542a6af2c6afd3143e952",
      "62572e0ffe3d4a7e86b9408a053fd843",
      "978d253cd2ec41b0b021fd6c734e133d",
      "624a72e70b73402b9cd74cea3457d9f6",
      "126b5757971a47a6868cdd15b67b0e1a",
      "7f4cd2235e6a43c9a9bb56a1c89671a1",
      "924b560bd4744de685cf126d1fd5826c",
      "9e4ac233487842bcb0c89bd88349b712",
      "673771e073c3470598f166c47daa0113",
      "1f49e12b1229412ba9df217d89651072",
      "87b34cd3bf024db5b715fb42d5d516a4",
      "376f849a805f449188523d754052dd4e",
      "3593967241824d19a8f9b7d35d378b68",
      "6d73b62c4b834258b7ecac03d78b7b26",
      "6e0d02d2add84316bf61b83433a2878b",
      "f13f0f4de84f42e18151a3e3f11a1fa1",
      "a08ffc7879514296a93982de26c761ff",
      "f02bf44d37234f9887325731c34f0957",
      "8c359e63ad1b4e848a36bd0f55b8300e",
      "dd2f5d3aba204c248b354bac670e69d6",
      "db0d3d1e86f1410a9ee1a080bb635398",
      "cd80c9e35682422da40a5262a1990c1b",
      "eaafa1579d464cae933d0da031202a80",
      "769ae7f0411e4fb48e2e2b16030ebefe",
      "14979b1e7fef441c89dd3b15467c3bc8",
      "651c5b8350264d37936bfe413c94d1bd",
      "7a68892d161944798d1a7f6963ce0d6c",
      "52c3e85d86084dc89c08d0b04aa6dec6",
      "c1933c4ddd604bf6adb3f4d7dd4b96fc",
      "65a7f3c8689e49fabadf21488454958c",
      "52e0e15ae85f43638008673c96ae04a2",
      "306cf8f372a94322bc1ae15dd120ee10",
      "82690b5eedbb4f23b1b218f36744afe7",
      "d1788db2642544c0841b0009dbf15258",
      "7d0d36c531b547a8b1313d4cb83a2b7f",
      "82780cc06a2e4c29936b1c79338974ec",
      "e15470fa99394fbf8aced92f9fa008fa",
      "da5b3cd9bca44091bb59b6ecbc8ab996",
      "f882c216a79d4aeba4cca0a7d8d8abcd",
      "4fe7b5bf760c4d6abb7fc6a543a1edb5",
      "73d181179611478c98228bd908ce0eb8",
      "94091ffb3cac492f945b2f7c8b4ab7ca",
      "9df8ec5ffd1d41d0bc8e4c04abd2906c",
      "3df8a0ef470f4bff9085f43525da2819",
      "e0bf4227bb2641e3ba488c5478191a91",
      "7c28f4695e5740c58e0c3003ccf36048",
      "59b95ff1e52e432b8470d797569812db",
      "4308b05b14964d0db543785e17ef72df",
      "4c9a74b79ada4e4b9e7e6ba01ef52201",
      "456454a41dc44e1fa9dba9ec4183f222",
      "2fcece316eff4bffa6cf95d935b49ce8",
      "b663040d0b0c45acaaac6c93b8c6235e",
      "7d18203c71c2413a8cae5ff0bffa823d",
      "9ecaa120672b4c96b88cb33585979cf7",
      "ecb0b57efc1f49759724678346429182",
      "ea88d4f1483f41918761dcc3924776bc",
      "ffe9e87cc7b947bc8dd565581a9fe2b4",
      "52bfbb93b9a74857be731f37ea467a53",
      "26f87430ffec44ecb02feb63d49ec074",
      "519d7643c71a469aafdb32e3dcb615c7",
      "cb42a1b4c7a9409e96b3c3792e63ad83",
      "03145f1aa32445bf80b0853515108a53",
      "699e6a68d90e4ce599b5399f86c9aa6e",
      "920bc22d813e4239800d0ac1656232f6",
      "a4a77015a7434d19a4632089ffd96ffa",
      "ef4ea81b78684a84b937c089884c0a4a",
      "535367156e584d3dac98ea3033770e36",
      "7927e91a06f94d84849d838912f73dd4",
      "4ce806c8ca194f9c832b0416e761729c",
      "c41af514883a4f72abfdba4772f2de1b",
      "677923ac3a7746599a33f9dbbb38f80e",
      "11067c35b4ad4189adcef1d41fa9f95d",
      "0f9086778b4f42b8b1d87b0166f53b6f",
      "498af3d8c5e6492c8184095d2c943852",
      "3f1be9e8c86d45dabe3b72fb843088b3",
      "84812bac35e8475e8ca8b8b432ddc91c"
     ]
    },
    "id": "aY5nOk2eCb3r",
    "outputId": "8a4f3cf2-df2d-4b43-91b6-6a874ecb6df8"
   },
   "outputs": [],
   "source": [
    "embeddings_improve = HuggingFaceEmbeddings(model_name=\"intfloat/e5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxglqOLGDaRm"
   },
   "outputs": [],
   "source": [
    "vector_store_improve = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings_improve,\n",
    "    metadatas=metadatas,\n",
    "    persist_directory=\"./improved_chroma_data\"\n",
    ")\n",
    "\n",
    "# Create the updated retriever\n",
    "retriever_improve = vector_store_improve.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS_w_Xm5IbHO"
   },
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain\n",
    "rag_chain_improve = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever_improve,\n",
    "    llm=lm,\n",
    "    chain_type=\"stuff\",       # Combine context and LM generation\n",
    "    return_source_documents=True,  # Return retrieved documents\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJUlfg2EWVRl"
   },
   "outputs": [],
   "source": [
    "responses_imp, predictions_imp, all_doc_ids_imp = compute_performance(rag_chain_improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn1U0GBQJW2Q",
    "outputId": "6cf97857-7221-4c31-9fbb-273c5bec9787"
   },
   "outputs": [],
   "source": [
    "# Evaluate F1 score for improvement1\n",
    "f1_imp = evaluate_generator(predictions_imp, gold_labels)\n",
    "print(f\"F1 Score: {f1_imp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnSDLO21Jyp-"
   },
   "outputs": [],
   "source": [
    "#====================================================\n",
    "# Improvement 2 - Change Chunker(Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyPAtnWYay3c",
    "outputId": "c6d06861-fe06-44aa-ca33-e7d7848d8fad"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YYnQwiebYbl"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(embeddings_improve, breakpoint_threshold_type=\"interquartile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bPohyvwbyOE"
   },
   "outputs": [],
   "source": [
    "semantic_chunked_docs = []\n",
    "for idx, row in documents.iterrows():\n",
    "    chunks = semantic_chunker.split_text(row['abstract'])\n",
    "    for chunk in chunks:\n",
    "        semantic_chunked_docs.append({\"doc_id\": idx, \"chunk\": chunk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etOragSpb_Aj",
    "outputId": "17726585-e41f-4ad9-9e5c-815a90bad345"
   },
   "outputs": [],
   "source": [
    "print(\"Sample Semantic Chunks:\")\n",
    "for i in range(10):\n",
    "    print(f\"Doc ID: {semantic_chunked_docs[i]['doc_id']}\")\n",
    "    print(f\"Chunk: {semantic_chunked_docs[i]['chunk']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-0cwkS3eanl"
   },
   "outputs": [],
   "source": [
    "texts_imp2 = [chunk[\"chunk\"] for chunk in semantic_chunked_docs]\n",
    "metadatas_imp2 = [{\"doc_id\": chunk[\"doc_id\"]} for chunk in semantic_chunked_docs]\n",
    "\n",
    "# Initialize Chroma vector store\n",
    "vector_store_imp2 = Chroma.from_texts(\n",
    "    texts=texts_imp2,         # List of document chunks\n",
    "    embedding=embeddings_improve,   # HuggingFace embedding model\n",
    "    metadatas=metadatas_imp2,     # Metadata associated with each document chunk\n",
    "    persist_directory=\"./improved_chroma_data2\"\n",
    ")\n",
    "\n",
    "retriever_imp2 = vector_store_imp2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeMyHBzqepVK"
   },
   "outputs": [],
   "source": [
    "# Create the RetrievalQA chain\n",
    "rag_chain_improve2 = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever_imp2,\n",
    "    llm=lm,\n",
    "    chain_type=\"stuff\",         # Combine context and LM generation\n",
    "    return_source_documents=True,    # Return retrieved documents\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_mJbiyZewCz"
   },
   "outputs": [],
   "source": [
    "responses_imp2, predictions_imp2, all_doc_ids_imp2 = compute_performance(rag_chain_improve2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXBcRFcweyZk",
    "outputId": "d3d0fc06-e606-456f-becc-73965e11d625"
   },
   "outputs": [],
   "source": [
    "# Evaluate F1 score for improvement 2\n",
    "f1_imp2 = evaluate_generator(predictions_imp2, gold_labels)\n",
    "print(f\"F1 Score: {f1_imp2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSasYAlcCQfg"
   },
   "outputs": [],
   "source": [
    "#====================================================\n",
    "# Improvement 3 - Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYy6_doFCoMV"
   },
   "outputs": [],
   "source": [
    "prompt_improve = ChatPromptTemplate.from_template(\n",
    "    template=\"\"\"You are a medical expert tasked with answering questions based on provided research abstracts.\n",
    "    Given the following context, answer the question with 'Yes' or 'No' only. Do not provide any explanation or additional information.\n",
    "    If the context does not contain enough evidence to support a 'Yes' answer, respond with 'No'.\n",
    "\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "\n",
    "    Answer (Yes or No):\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t9XauQKCUvV"
   },
   "outputs": [],
   "source": [
    "rag_chain_improve3 = RetrievalQA.from_chain_type(\n",
    "    retriever=retriever,\n",
    "    llm=lm,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_improve}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zP0Oa5yAERuw",
    "outputId": "4baf6324-c4db-44a1-8537-92a22ff2bf5a"
   },
   "outputs": [],
   "source": [
    "test_input = {\"query\": \"Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\"}\n",
    "response = rag_chain_improve3.invoke(test_input)\n",
    "\n",
    "\n",
    "print(f\"--- Test Question ---\")\n",
    "print(f\"Answer: {response['result']}\")\n",
    "print(f\"Retrieved Document: {response['source_documents']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWJleTA5DAu1"
   },
   "outputs": [],
   "source": [
    "responses_imp3, predictions_imp3, all_doc_ids_imp3 = compute_performance(rag_chain_improve3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7aQFyvFDHR2",
    "outputId": "e70f6454-648c-41e5-d42a-26657719775d"
   },
   "outputs": [],
   "source": [
    "# Evaluate F1 score for improvement 3\n",
    "f1_imp3 = evaluate_generator(predictions_imp3, gold_labels)\n",
    "print(f\"F1 Score: {f1_imp3:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
